样本->变换、数字化（特征）->预处理（第七章）->特征选择（过滤不必要、影响小的特征）->模型选择（样本可视化后观测大概规律选择模型）->训练（k折交叉验证避免过拟合）->存储及应用（回归，分类）->结果可视化（通过B/S或C/S图表）

因为训练的时间、人力和物质成本导致模型很贵，训练时间可能达到日、月

回归分析，统计分类

算法原理
算法应用
算法优缺点评估

评价指标：
（回归）
MSE
MAE
（分类）
混淆矩阵
TP FN
FP TN
pre(精准率) = TP/(TP+FP)
Acc(准确率) = (TP+TN) / (所有)
** R(召回率、查全率) = TP/(TP+FN)

ROC曲线 (FPR, TPR)
FPR:阴判成阳 FN/(TN+FP)
TPR：阳正确判断为阳
AUC(area under curve ) = (1 + TPR - FPR) / 2
随机猜测AUC=0.5

训练集和测试集精度不超过5%，不然认为过拟合
理论不能出现测试集比训练集精度高，可能欠拟合
...等等...

实际中处理的是非确定性关系，两个变量在公关上存在关系，但未经确找到可用函数关系表达，但是尽量向确定性关系（有准确的函数关系）靠拢
y^=a+bx 为非确定性关系
y=a+bx 未确定性关系

训练实际是找a和b的参数
最优解通过最小二乘法保证平方和最小（每个点到线上的垂直[纵向]距离最短）即损失函数（Y-Y^为残差）loss = min(∑(Y-Y^)²=∑[Yi-(a+bxi)]²) 平均的损失函数1/N*loss 实际不可能为0，为0即在线上
1/N不影响其趋势
xi yi 已知，极值存在于一阶偏导数=0的点，（同时对a,b求偏导，且同时为0的点）


1-多元线性回归
Y不仅仅取决于一个X
而是Y=b0+b1x1+b2x2+...+e  e为随机残差（噪声）
还是分别对所有系数求偏导同时等于0的点

2-KNN——找最好的K，几个邻居效果最好（分类）
找出前k个距离最短

相似度体现在某些维度上的距离
求距离几种方法：
1.欧氏距离:((x1-x2)^2 + (y1-y2)^2)^1/2 ，但是可能弱化某些特征的权重，差很小但是特征很明显，所以可在每项前面除以权值标准化特征，标准化欧氏距离
2.归一化，放在[0,1]中
3.曼哈顿距离
--------
|-|-|-|-|
|网格-  |
--------
只能走横向或纵向，地图距离使用曼哈顿，因为基本不可能走直线

马氏距离使用协方差矩阵计算
人脸128个特征可视化分析出那些特征几乎无差别并剔除

劣势：样本不平衡，样本分布不均匀
》》》解决：1.动态权值，越近的距离权值越高
     样本增多计算量大，与每一个样本一一计算距离
》》》解决：1.分组快速搜索近邻法，找每一组的中心（质心、重心）[但是，边界上样本点存在感太模糊]

Logistic回归(分类)
hθ(x) = θ0 + θ1*x1 + ... + θn*xn
θ = [θ0,...θn]
将回归问题使用sigmod函数计算变作分类问题
将所有值放在0-1之间，y是距离1的概率
hθ(x) = g(θTx)
g(z) = 1/1+e^(-z)
p(y|x,θ) = (hθ(x))^y*(1-hθ(x))^(1-y) 【y=1或0】
p(y=0|x,θ) + p(y=1|x,θ) = 1


1-求极大似然函数
L(θ) = ∏p(yi|xi,θ) 求极值不好求去对数求和变成
l(θ) = log(L(θ))
2-梯度下降法
训练一组最优θ,目标是最小J(θ)
J(θ) = 1/m∑1/2*(hθ(xi)-yi^2)
J(θ) = 1/m∑1/2*Cost(hθ(xi),yi)
repeat{
    梯度下降：θj等于θj-1减去一个α*J(θ)对θj求导,进行同步更新simultaneously update α是学习率，或步长快速找到极值点，α步长大小很关键不能太大不能太小跨过或者太慢，当导数为0停止或者设置迭代次数
}
（α过大过了极值点会来回震荡，因为根据梯度（斜率、导数）决定下一步怎么走，导数<0向前走 导数>0向后走）

优点：
计算代价不高，相对于knn使用了sigmod
缺点：
分类精度不高，容易欠拟合：取到0和1需要巨大的样本，因为趋近无穷大才会收敛于1或0，但是实际样本有限

k-means (K均值)  根据means 平均值
J(c,μ) = ∑||x - μci||²  每一个样本与各自类的质心算欧氏距离
c -> 一个类， μ -> 一个质心

1.随机选取k个质心点（选择质心和关键，影响最后收敛结果）
2.repeat{
    计算各个样本与每个质心的距离；（一次迭代是计算完所有样本点，下一次再次算所有样本点）
    将样本点归于“距离”最近的类;
    因为归了新样本进入，重新计算此类质心位置;
}（直到质心计算函数收敛:加入新样本后，质心位置“不变”）

迭代次数过多：可设置迭代一定次数以及设置收敛值近似即可停止
避免收敛于局部的问题采用二分思想，二分K均值算法，降低SSE（族内）
reapeat{
    二分；
    在最大的区域中再划分
}

训练过程找最小的μ，是
-----------
多类分类基本是二分类，1VR
不同的算法的区别是划分区域的方式，而SVM是找一个超平面
SVM
wx + b = 0
w -> 法向量 Ax+By+Cz=0一个经过原点的平面，在高中时期关注的是平面之间的关系，并没有确定一个平面，已知一个法向量，为了确定平面在空间的位置还需要一个平面偏移量
而确定一个超平面可通过两个超平面确定一个中间的平面 wx+b=c和wx+b=-c,同时除以c,将w/c代还为w，原式wx+b=0除以c也没有变化,为了方便计算两个平面变成wx+b=±1